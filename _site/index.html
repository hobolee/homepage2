<!DOCTYPE html>
<html lang="en-US">
  <head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="application-name" content="Linping YUAN">
  <meta name="theme-color" content="#b00">
  <meta name="keywords" content="袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping">
  <meta name="description" content="Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.">
  <title>Linping YUAN</title>
  <meta property="og:title" content="Linping YUAN">
  <meta property="og:site_name" content="http://localhost:4000 Linping YUAN">
  <meta property="og:type" content="article">
  <meta property="og:url" content="http://localhost:4000">
  <meta property="og:image" content="">
  <meta property="og:description" content="Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.">
  <meta name="author" content="Linping YUAN (袁林萍)">
  <link rel="shortcut icon" href="/favicon.ico"/>
  <link rel="icon" type="image/png" href="/favicon.png" sizes="250x250" />

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Home | Linping YUAN</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Home" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization." />
<meta property="og:description" content="I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization." />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="Linping YUAN" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Home" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.","headline":"Home","name":"Linping YUAN","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->


  <link rel="alternate" type="application/rss+xml" title="Linping YUAN" href="http://localhost:4000/feed.xml">

  <link href="https://use.fontawesome.com/releases/v5.15.3/css/all.css" rel="stylesheet">
  <link href="/styles.css" rel="stylesheet">
</head>
  
  <body>
    
<header class="page-header">
  <nav class="container">
    <a class="site-title" href="/">Linping YUAN</a>

    <a href="/publications/">Publications</a>
    <a href="/projects/">Projects</a>
    <a href="/cv/">CV</a>

    <span class="external">
      <a href="https://yuanlinping.github.io/blog"> Blog</a>
    </span>
  </nav>
</header>


    <div class="page-content">
      <section class="container home">
  <div class="columns">

  <div class="intro">

    <p>I am Linping YUAN (袁林萍), a Ph.D. candidate at <a href="http://vis.cse.ust.hk/">HKUST VisLab</a>, at the Department of Computer Science and Engineering of the <a href="https://hkust.edu.hk/">Hong Kong University of Science and Technology (HKUST)</a>, supervised by Prof. <a href="http://www.huamin.org/">Huamin Qu</a>. Before that, I obtained my bachelor’s degree in Software Engineering from <a href="http://en.xjtu.edu.cn/">Xi’an Jiaotong University (XJTU)</a> in 2019.</p>

    <p>My research interests include Augmented Reality (AR), Human-Computer Interaction (HCI), and Data Visualizations.</p>

    <p>In the first two years of my Ph.D. journey, I focused on how to leverage deep learning methods to facilitate the design of data visualizations and infographics, with an emphasis on color design. I have published two first-author papers on this topic in a top journal.</p>

  </div>

  <div class="me">
    <picture>
  <source srcset="/images/linping_profile.png" type="image/png"></source>
  <img src="/images/linping_profile.png" alt="Linping YUAN">
</picture>

    <!-- find icons here: https://www.angularjswiki.com/fontawesome/ -->
    <ul class="no-list">
      <li>
<i class="fa fa-envelope"></i> <a href="mailto:lyuanaa@connect.ust.hk"> lyuanaa@connect.ust.hk</a>
</li>
      <li>
<i class="fab fa-github"></i> <a href="https://github.com/yuanlinping"> GitHub</a>
</li>
      <li>
<i class="fab fa-google"></i> <a href="https://scholar.google.com/citations?user=xp3QhPkAAAAJ&amp;hl=en">Google Scholar</a>
</li>
    </ul>
  </div>

</div>

<div class="columns">
  <div class="intro">
    <p>Aiming to explore more unknowns and challenge myself, I shifted my interests to AR/VR and have been exploring the intersection of AR/VR, HCI, and visualization since 2020 summer. Currently, I am a core member of <a href="http://vis.cse.ust.hk/groups/xr-vis/">VisLab AR/VR Team</a>. My ongoing research projects aim to build an AR/VR-enhanced campus and enrich HKUST members’ living experience with AR/VR techniques and applications.</p>
  </div>
</div>

<h2 id="latest-news">Latest News</h2>

<div class="news">
  <table>
<tbody>

  <tr>
  <td class="date">
    <time datetime="2022-04-28">Apr 2022</time>
  </td>
  <td>
    We won the Deloitte ESG Innovation Award in HackUST 2022 (Top prize under the ESG theme). Further details can be found <a href="https://jasonwong.vision/projects/vispie/">here</a>.

  </td>
</tr>

  <tr>
  <td class="date">
    <time datetime="2021-08-24">Aug 2021</time>
  </td>
  <td>
    I pass the Ph.D. Qualification Exam! I am a Ph.D. candidate now. My PQE topic is about using AR for real-time tasks and the design of AR user interfaces. The slides can be found <a href="/assets/slide/2021_pqe_slide.pdf">here</a>.

  </td>
</tr>

  <tr>
  <td class="date">
    <time datetime="2021-07-20">Jul 2021</time>
  </td>
  <td>
    My visit to the <a href="https://www.zhejianglab.com/">Zhejiang Lab</a> under the supervision of <a href="http://www.ycwu.org/">Prof. Yingcai Wu</a> is over. See you, friends!

  </td>
</tr>

  <tr>
  <td class="date">
    <time datetime="2021-06-01">Jun 2021</time>
  </td>
  <td>
    Our paper <em>InfoColorizer Interactive Recommendation of Color Palettes for Infographics</em> is accepted by TVCG. The preprint version can be found <a href="/assets/paper/2021_infocolorizer.pdf">here</a>.

  </td>
</tr>

  <tr>
  <td class="date">
    <time datetime="2021-04-05">Apr 2021</time>
  </td>
  <td>
    Our paper <em>Deep Colormap Extraction from Visualizations</em> is accepted by TVCG. The preprint version can be found <a href="/assets/paper/2021_deep_colormap.pdf">here</a>.

  </td>
</tr>

</tbody>
</table>
</div>

<h2 id="featured-projects">Featured Projects</h2>

<div class="featured-projects">
  
  
    
  
    
  
    
  
    
      <div class="project">
  <span class="title">VisPIE</span>
  
    <a href="https://jasonwong.vision/projects/vispie/" class="preview-image" style="background-image: url('/assets/project/2022_vispie.png')"></a>
    
  <p>
    ViePIE aims to promote sustainable lifestyle with AR gamification and digital twin. We enable individuals to perceive the environmental implications of their measurable activities, immerse themselves in the climate change impacts, and empower both individuals and organizations with climate-smart choices.

  </p>
  <div class="spacer"></div>
  <div class="links">
    
    
      <a href="https://hkustconnect-my.sharepoint.com/:v:/g/personal/lyuanaa_connect_ust_hk/EQ2H2TTvQlBMlodzKd0FdKUBlJcFo4ae8UjfD-QUH4tKiQ?e=Rzfemt"><i class="fas fa-play"></i> Demo</a>
    
    
      <a href="https://jasonwong.vision/projects/vispie/"><i class="fas fa-link" aria-hidden="true"></i> Website</a>
    
    
      <a href="https://github.com/wtong2017/hackust2022-vispie"><i class="fab fa-github" aria-hidden="true"></i> Code</a>
    
    
    
    
      <a href=""><i class="fas fa-video"></i> Talk</a>
    
    
    <a href=""><i class="fas fa-window-maximize" aria-hidden="true"></i> Slides</a>
    
  </div>
</div>

    
  
    
      <div class="project">
  <span class="title">InfoColorizer</span>
  
    <a href="https://doi.org/10.1109/TVCG.2021.3085327" class="preview-image" style="background-image: url('/assets/project/2021_infocolorizer.png')"></a>
    
  <p>
    InfoColorizer is an interactive tool to facilitate color palette creation for infographics with deep learning techniques. It provides novice users with flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements.

  </p>
  <div class="spacer"></div>
  <div class="links">
    
    
    
    
      <a href="https://github.com/yuanlinping/InfoColorizer"><i class="fab fa-github" aria-hidden="true"></i> Code</a>
    
    
      <a href="https://doi.org/10.1109/TVCG.2021.3085327"><i class="far fa-file-pdf"></i> Paper</a>
    
    
      <a href="https://www.youtube.com/watch?v=FZvLt0AAIAI"><i class="fas fa-film"></i> Video</a>
    
    
      <a href=""><i class="fas fa-video"></i> Talk</a>
    
    
  </div>
</div>

    
  
    
      <div class="project">
  <span class="title">Color Extractor</span>
  
    <a href="https://doi.org/10.1109/TVCG.2021.3070876" class="preview-image" style="background-image: url('/assets/project/2021_deep_colormap.png')"></a>
    
  <p>
    This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. With the extracted colormaps, we have developed two applications, i.e., color transfer and color remapping.

  </p>
  <div class="spacer"></div>
  <div class="links">
    
    
    
    
      <a href="https://github.com/yuanlinping/deep_colormap_extraction"><i class="fab fa-github" aria-hidden="true"></i> Code</a>
    
    
      <a href="https://doi.org/10.1109/TVCG.2021.3070876"><i class="far fa-file-pdf"></i> Paper</a>
    
    
    
    
  </div>
</div>

    
  
</div>
<p><a href="/projects/" class="button">
  <i class="fas fa-chevron-circle-right"></i>
  Show More Projects
</a></p>

<h2 id="featured-publications">Featured Publications</h2>

<!-- style 1: with border -->
<div class="pubs">
  
  
    
      

<div class="publication" data-pub="{&quot;relative_path&quot;:&quot;_publications/2021_infocolorizer.html&quot;,&quot;excerpt&quot;:&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:null,&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;excerpt&quot;:&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2021_infocolorizer.html&quot;,&quot;excerpt&quot;:&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:null,&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;url&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;Deep Colormap Extraction from Visualizations&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Wei Zeng&quot;,&quot;Siwei Fu&quot;,&quot;Zhiliang Zeng&quot;,&quot;Haotian Li&quot;,&quot;Chi-Wing Fu&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2021,&quot;venue&quot;:&quot;IEEE Transactions on Visualization and Computer Graphics&quot;,&quot;venue_tags&quot;:[&quot;TVCG&quot;],&quot;type&quot;:[&quot;Journal&quot;],&quot;tags&quot;:[&quot;Color&quot;,&quot;Visualization&quot;,&quot;AI for X&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/TVCG.2021.3070876&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2021_deep_colormap.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2021_deep_colormap.pdf&quot;,&quot;appendix&quot;:&quot;/assets/appendix/2021_deep_colormap_appendix.pdf&quot;,&quot;code&quot;:&quot;https://github.com/yuanlinping/deep_colormap_extraction&quot;,&quot;arxiv&quot;:2103.00741,&quot;slug&quot;:&quot;2021_deep_colormap&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2021_infocolorizer&quot;,&quot;content&quot;:&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Linping Yuan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021_infocolorizer\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021_infocolorizer\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_infocolorizer.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_infocolorizer.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Linping Yuan\&quot;},\&quot;description\&quot;:\&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.\&quot;,\&quot;headline\&quot;:\&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2021_infocolorizer.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021_infocolorizer\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2021_infocolorizer.html&quot;,&quot;url&quot;:&quot;/publications/2021_infocolorizer&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Ziqi Zhou&quot;,&quot;Jian Zhao&quot;,&quot;Yiqiu Guo&quot;,&quot;Fan Du&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2021,&quot;venue&quot;:&quot;IEEE Transactions on Visualization and Computer Graphics&quot;,&quot;venue_tags&quot;:[&quot;TVCG&quot;],&quot;type&quot;:[&quot;Journal&quot;],&quot;tags&quot;:[&quot;AI for X&quot;,&quot;Graphic design&quot;,&quot;Color&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/TVCG.2021.3085327&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2021_infocolorizer.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2021_infocolorizer.pdf&quot;,&quot;appendix&quot;:&quot;/assets/appendix/2021_infocolorizer_appendix.pdf&quot;,&quot;code&quot;:&quot;https://github.com/yuanlinping/InfoColorizer&quot;,&quot;demo&quot;:&quot;https://www.youtube.com/watch?v=FZvLt0AAIAI&quot;,&quot;arxiv&quot;:2102.02041,&quot;slides&quot;:&quot;/assets/slide/2021_infocolorizer_slide.pdf&quot;,&quot;talk&quot;:&quot;https://www.youtube.com/watch?v=z23Zq5kZruE&quot;,&quot;slug&quot;:&quot;2021_infocolorizer&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;excerpt&quot;:&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;url&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;Deep Colormap Extraction from Visualizations&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Wei Zeng&quot;,&quot;Siwei Fu&quot;,&quot;Zhiliang Zeng&quot;,&quot;Haotian Li&quot;,&quot;Chi-Wing Fu&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2021,&quot;venue&quot;:&quot;IEEE Transactions on Visualization and Computer Graphics&quot;,&quot;venue_tags&quot;:[&quot;TVCG&quot;],&quot;type&quot;:[&quot;Journal&quot;],&quot;tags&quot;:[&quot;Color&quot;,&quot;Visualization&quot;,&quot;AI for X&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/TVCG.2021.3070876&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2021_deep_colormap.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2021_deep_colormap.pdf&quot;,&quot;appendix&quot;:&quot;/assets/appendix/2021_deep_colormap_appendix.pdf&quot;,&quot;code&quot;:&quot;https://github.com/yuanlinping/deep_colormap_extraction&quot;,&quot;arxiv&quot;:2103.00741,&quot;slug&quot;:&quot;2021_deep_colormap&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2019_speechlens.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2019_speechlens&quot;,&quot;path&quot;:&quot;_publications/2019_speechlens.html&quot;,&quot;url&quot;:&quot;/publications/2019_speechlens&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Yuanzhe Chen&quot;,&quot;Siwei Fu&quot;,&quot;Aoyu Wu&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2019,&quot;venue&quot;:&quot;IEEE International Conference on Big Data and Smart Computing&quot;,&quot;venue_location&quot;:&quot;Kyoto, Japan&quot;,&quot;venue_tags&quot;:[&quot;BigComp&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Multimodal analysis&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/BIGCOMP.2019.8679261&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2019_speechlens.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2019_speechlens.pdf&quot;,&quot;demo&quot;:&quot;https://www.youtube.com/watch?v=dtv03qEVFDM&amp;t=1s&quot;,&quot;slug&quot;:&quot;2019_speechlens&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2020_ice&quot;,&quot;content&quot;:&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Siwei Fu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020_ice\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020_ice\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2020_ice.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2020_ice.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Siwei Fu\&quot;},\&quot;description\&quot;:\&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.\&quot;,\&quot;headline\&quot;:\&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2020_ice.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020_ice\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;url&quot;:&quot;/publications/2020_ice&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations&quot;,&quot;authors&quot;:[&quot;Siwei Fu&quot;,&quot;Jian Zhao&quot;,&quot;Linping Yuan&quot;,&quot;Zhicheng Liu&quot;,&quot;Kwan-Liu Ma&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2020,&quot;venue&quot;:&quot;arXiv preprint&quot;,&quot;venue_tags&quot;:[&quot;arxiv&quot;],&quot;type&quot;:[&quot;preprint&quot;],&quot;tags&quot;:[&quot;Sequence analysis&quot;,&quot;Visual analytics&quot;],&quot;highlight&quot;:false,&quot;arxiv&quot;:2006.12718,&quot;image&quot;:&quot;/assets/paper_teaser/2020_ice.png&quot;,&quot;slug&quot;:&quot;2020_ice&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;content&quot;:&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;Deep Colormap Extraction from Visualizations | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Deep Colormap Extraction from Visualizations\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Linping Yuan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Deep Colormap Extraction from Visualizations\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Linping Yuan\&quot;},\&quot;description\&quot;:\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot;,\&quot;headline\&quot;:\&quot;Deep Colormap Extraction from Visualizations\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;url&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;Deep Colormap Extraction from Visualizations&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Wei Zeng&quot;,&quot;Siwei Fu&quot;,&quot;Zhiliang Zeng&quot;,&quot;Haotian Li&quot;,&quot;Chi-Wing Fu&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2021,&quot;venue&quot;:&quot;IEEE Transactions on Visualization and Computer Graphics&quot;,&quot;venue_tags&quot;:[&quot;TVCG&quot;],&quot;type&quot;:[&quot;Journal&quot;],&quot;tags&quot;:[&quot;Color&quot;,&quot;Visualization&quot;,&quot;AI for X&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/TVCG.2021.3070876&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2021_deep_colormap.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2021_deep_colormap.pdf&quot;,&quot;appendix&quot;:&quot;/assets/appendix/2021_deep_colormap_appendix.pdf&quot;,&quot;code&quot;:&quot;https://github.com/yuanlinping/deep_colormap_extraction&quot;,&quot;arxiv&quot;:2103.00741,&quot;slug&quot;:&quot;2021_deep_colormap&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2021_infocolorizer&quot;,&quot;content&quot;:&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Linping Yuan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021_infocolorizer\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021_infocolorizer\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_infocolorizer.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_infocolorizer.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Linping Yuan\&quot;},\&quot;description\&quot;:\&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.\&quot;,\&quot;headline\&quot;:\&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2021_infocolorizer.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021_infocolorizer\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2021_infocolorizer.html&quot;,&quot;url&quot;:&quot;/publications/2021_infocolorizer&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Ziqi Zhou&quot;,&quot;Jian Zhao&quot;,&quot;Yiqiu Guo&quot;,&quot;Fan Du&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2021,&quot;venue&quot;:&quot;IEEE Transactions on Visualization and Computer Graphics&quot;,&quot;venue_tags&quot;:[&quot;TVCG&quot;],&quot;type&quot;:[&quot;Journal&quot;],&quot;tags&quot;:[&quot;AI for X&quot;,&quot;Graphic design&quot;,&quot;Color&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/TVCG.2021.3085327&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2021_infocolorizer.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2021_infocolorizer.pdf&quot;,&quot;appendix&quot;:&quot;/assets/appendix/2021_infocolorizer_appendix.pdf&quot;,&quot;code&quot;:&quot;https://github.com/yuanlinping/InfoColorizer&quot;,&quot;demo&quot;:&quot;https://www.youtube.com/watch?v=FZvLt0AAIAI&quot;,&quot;arxiv&quot;:2102.02041,&quot;slides&quot;:&quot;/assets/slide/2021_infocolorizer_slide.pdf&quot;,&quot;talk&quot;:&quot;https://www.youtube.com/watch?v=z23Zq5kZruE&quot;,&quot;slug&quot;:&quot;2021_infocolorizer&quot;,&quot;ext&quot;:&quot;.html&quot;}">
  <div class="thumbnail">
    <img src="/assets/paper_teaser/2021_infocolorizer.png">
  </div>
  <div class="publication-content">
    <h3 id="/publications/2021_infocolorizer">InfoColorizer: Interactive Recommendation of Color Palettes for Infographics</h3>
    <div class="authors">
      <a href="https://yuanlinping.github.io/"><strong>Linping Yuan</strong></a>, 
      Ziqi Zhou, 
      <a href="http://www.jeffjianzhao.com/">Jian Zhao</a>, 
      Yiqiu Guo, 
      <a href="http://frankdu.org/">Fan Du</a>, 
      <a href="http://www.huamin.org/">Huamin Qu</a>
      
    </div>
    
      <div class="venue">
        
        IEEE Transactions on Visualization and Computer Graphics (2021)
        
      </div>
    

    

    

    
      <div class="extra-links">
      
        <a target="_blank" href="https://doi.org/10.1109/TVCG.2021.3085327">
            <i class="fas fa-book" aria-hidden="true"></i> DOI
        </a>
      
      
        <a href="/assets/paper/2021_infocolorizer.pdf">
          <i class="far fa-file-pdf" aria-hidden="true"></i> PDF
        </a>
      
      
      <a href="/assets/appendix/2021_infocolorizer_appendix.pdf">
        <i class="far fa-file-pdf" aria-hidden="true"></i> Appendix
      </a>
      
      
      
      <a href="https://github.com/yuanlinping/InfoColorizer">
        <i class="fab fa-github" aria-hidden="true"></i> Code
      </a>
    
      
      
        <a href="https://www.youtube.com/watch?v=FZvLt0AAIAI">
          <i class="fas fa-play" aria-hidden="true"></i> Demo
        </a>
      
      
        <a href="https://www.youtube.com/watch?v=z23Zq5kZruE">
          <i class="fas fa-video" aria-hidden="true"></i> Talk
        </a>
      
      
        <a href="/assets/slide/2021_infocolorizer_slide.pdf">
          <i class="fas fa-window-maximize" aria-hidden="true"></i> Slides
        </a>
      
      
      <a href="https://arxiv.org/abs/2102.02041">
        <i class="fas fa-archive" aria-hidden="true"></i> arXiv
      </a>
      
      
      
      </div>
    
  </div>
</div>

    
  
    
      

<div class="publication" data-pub="{&quot;relative_path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;excerpt&quot;:&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2021_infocolorizer.html&quot;,&quot;excerpt&quot;:&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:null,&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;excerpt&quot;:&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2021_infocolorizer.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2021_infocolorizer&quot;,&quot;path&quot;:&quot;_publications/2021_infocolorizer.html&quot;,&quot;url&quot;:&quot;/publications/2021_infocolorizer&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Ziqi Zhou&quot;,&quot;Jian Zhao&quot;,&quot;Yiqiu Guo&quot;,&quot;Fan Du&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2021,&quot;venue&quot;:&quot;IEEE Transactions on Visualization and Computer Graphics&quot;,&quot;venue_tags&quot;:[&quot;TVCG&quot;],&quot;type&quot;:[&quot;Journal&quot;],&quot;tags&quot;:[&quot;AI for X&quot;,&quot;Graphic design&quot;,&quot;Color&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/TVCG.2021.3085327&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2021_infocolorizer.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2021_infocolorizer.pdf&quot;,&quot;appendix&quot;:&quot;/assets/appendix/2021_infocolorizer_appendix.pdf&quot;,&quot;code&quot;:&quot;https://github.com/yuanlinping/InfoColorizer&quot;,&quot;demo&quot;:&quot;https://www.youtube.com/watch?v=FZvLt0AAIAI&quot;,&quot;arxiv&quot;:2102.02041,&quot;slides&quot;:&quot;/assets/slide/2021_infocolorizer_slide.pdf&quot;,&quot;talk&quot;:&quot;https://www.youtube.com/watch?v=z23Zq5kZruE&quot;,&quot;slug&quot;:&quot;2021_infocolorizer&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2020_ice&quot;,&quot;path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;url&quot;:&quot;/publications/2020_ice&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations&quot;,&quot;authors&quot;:[&quot;Siwei Fu&quot;,&quot;Jian Zhao&quot;,&quot;Linping Yuan&quot;,&quot;Zhicheng Liu&quot;,&quot;Kwan-Liu Ma&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2020,&quot;venue&quot;:&quot;arXiv preprint&quot;,&quot;venue_tags&quot;:[&quot;arxiv&quot;],&quot;type&quot;:[&quot;preprint&quot;],&quot;tags&quot;:[&quot;Sequence analysis&quot;,&quot;Visual analytics&quot;],&quot;highlight&quot;:false,&quot;arxiv&quot;:2006.12718,&quot;image&quot;:&quot;/assets/paper_teaser/2020_ice.png&quot;,&quot;slug&quot;:&quot;2020_ice&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;content&quot;:&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;Deep Colormap Extraction from Visualizations | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Deep Colormap Extraction from Visualizations\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Linping Yuan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Deep Colormap Extraction from Visualizations\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Linping Yuan\&quot;},\&quot;description\&quot;:\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot;,\&quot;headline\&quot;:\&quot;Deep Colormap Extraction from Visualizations\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;url&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;Deep Colormap Extraction from Visualizations&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Wei Zeng&quot;,&quot;Siwei Fu&quot;,&quot;Zhiliang Zeng&quot;,&quot;Haotian Li&quot;,&quot;Chi-Wing Fu&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2021,&quot;venue&quot;:&quot;IEEE Transactions on Visualization and Computer Graphics&quot;,&quot;venue_tags&quot;:[&quot;TVCG&quot;],&quot;type&quot;:[&quot;Journal&quot;],&quot;tags&quot;:[&quot;Color&quot;,&quot;Visualization&quot;,&quot;AI for X&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/TVCG.2021.3070876&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2021_deep_colormap.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2021_deep_colormap.pdf&quot;,&quot;appendix&quot;:&quot;/assets/appendix/2021_deep_colormap_appendix.pdf&quot;,&quot;code&quot;:&quot;https://github.com/yuanlinping/deep_colormap_extraction&quot;,&quot;arxiv&quot;:2103.00741,&quot;slug&quot;:&quot;2021_deep_colormap&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2021_infocolorizer&quot;,&quot;content&quot;:&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Linping Yuan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021_infocolorizer\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021_infocolorizer\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_infocolorizer.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_infocolorizer.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Linping Yuan\&quot;},\&quot;description\&quot;:\&quot;When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.\&quot;,\&quot;headline\&quot;:\&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2021_infocolorizer.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021_infocolorizer\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements’ spatial arrangement. We propose a data-driven method that provides flexibility by considering users’ preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2021_infocolorizer.html&quot;,&quot;url&quot;:&quot;/publications/2021_infocolorizer&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Ziqi Zhou&quot;,&quot;Jian Zhao&quot;,&quot;Yiqiu Guo&quot;,&quot;Fan Du&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2021,&quot;venue&quot;:&quot;IEEE Transactions on Visualization and Computer Graphics&quot;,&quot;venue_tags&quot;:[&quot;TVCG&quot;],&quot;type&quot;:[&quot;Journal&quot;],&quot;tags&quot;:[&quot;AI for X&quot;,&quot;Graphic design&quot;,&quot;Color&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/TVCG.2021.3085327&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2021_infocolorizer.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2021_infocolorizer.pdf&quot;,&quot;appendix&quot;:&quot;/assets/appendix/2021_infocolorizer_appendix.pdf&quot;,&quot;code&quot;:&quot;https://github.com/yuanlinping/InfoColorizer&quot;,&quot;demo&quot;:&quot;https://www.youtube.com/watch?v=FZvLt0AAIAI&quot;,&quot;arxiv&quot;:2102.02041,&quot;slides&quot;:&quot;/assets/slide/2021_infocolorizer_slide.pdf&quot;,&quot;talk&quot;:&quot;https://www.youtube.com/watch?v=z23Zq5kZruE&quot;,&quot;slug&quot;:&quot;2021_infocolorizer&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;excerpt&quot;:&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;excerpt&quot;:&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2021_infocolorizer.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2021_infocolorizer&quot;,&quot;path&quot;:&quot;_publications/2021_infocolorizer.html&quot;,&quot;url&quot;:&quot;/publications/2021_infocolorizer&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Ziqi Zhou&quot;,&quot;Jian Zhao&quot;,&quot;Yiqiu Guo&quot;,&quot;Fan Du&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2021,&quot;venue&quot;:&quot;IEEE Transactions on Visualization and Computer Graphics&quot;,&quot;venue_tags&quot;:[&quot;TVCG&quot;],&quot;type&quot;:[&quot;Journal&quot;],&quot;tags&quot;:[&quot;AI for X&quot;,&quot;Graphic design&quot;,&quot;Color&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/TVCG.2021.3085327&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2021_infocolorizer.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2021_infocolorizer.pdf&quot;,&quot;appendix&quot;:&quot;/assets/appendix/2021_infocolorizer_appendix.pdf&quot;,&quot;code&quot;:&quot;https://github.com/yuanlinping/InfoColorizer&quot;,&quot;demo&quot;:&quot;https://www.youtube.com/watch?v=FZvLt0AAIAI&quot;,&quot;arxiv&quot;:2102.02041,&quot;slides&quot;:&quot;/assets/slide/2021_infocolorizer_slide.pdf&quot;,&quot;talk&quot;:&quot;https://www.youtube.com/watch?v=z23Zq5kZruE&quot;,&quot;slug&quot;:&quot;2021_infocolorizer&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2020_ice&quot;,&quot;path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;url&quot;:&quot;/publications/2020_ice&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations&quot;,&quot;authors&quot;:[&quot;Siwei Fu&quot;,&quot;Jian Zhao&quot;,&quot;Linping Yuan&quot;,&quot;Zhicheng Liu&quot;,&quot;Kwan-Liu Ma&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2020,&quot;venue&quot;:&quot;arXiv preprint&quot;,&quot;venue_tags&quot;:[&quot;arxiv&quot;],&quot;type&quot;:[&quot;preprint&quot;],&quot;tags&quot;:[&quot;Sequence analysis&quot;,&quot;Visual analytics&quot;],&quot;highlight&quot;:false,&quot;arxiv&quot;:2006.12718,&quot;image&quot;:&quot;/assets/paper_teaser/2020_ice.png&quot;,&quot;slug&quot;:&quot;2020_ice&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;content&quot;:&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;Deep Colormap Extraction from Visualizations | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Deep Colormap Extraction from Visualizations\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Linping Yuan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Deep Colormap Extraction from Visualizations\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Linping Yuan\&quot;},\&quot;description\&quot;:\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot;,\&quot;headline\&quot;:\&quot;Deep Colormap Extraction from Visualizations\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;url&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;Deep Colormap Extraction from Visualizations&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Wei Zeng&quot;,&quot;Siwei Fu&quot;,&quot;Zhiliang Zeng&quot;,&quot;Haotian Li&quot;,&quot;Chi-Wing Fu&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2021,&quot;venue&quot;:&quot;IEEE Transactions on Visualization and Computer Graphics&quot;,&quot;venue_tags&quot;:[&quot;TVCG&quot;],&quot;type&quot;:[&quot;Journal&quot;],&quot;tags&quot;:[&quot;Color&quot;,&quot;Visualization&quot;,&quot;AI for X&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/TVCG.2021.3070876&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2021_deep_colormap.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2021_deep_colormap.pdf&quot;,&quot;appendix&quot;:&quot;/assets/appendix/2021_deep_colormap_appendix.pdf&quot;,&quot;code&quot;:&quot;https://github.com/yuanlinping/deep_colormap_extraction&quot;,&quot;arxiv&quot;:2103.00741,&quot;slug&quot;:&quot;2021_deep_colormap&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2019_speechlens.html&quot;,&quot;excerpt&quot;:&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2020_ice&quot;,&quot;path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;url&quot;:&quot;/publications/2020_ice&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations&quot;,&quot;authors&quot;:[&quot;Siwei Fu&quot;,&quot;Jian Zhao&quot;,&quot;Linping Yuan&quot;,&quot;Zhicheng Liu&quot;,&quot;Kwan-Liu Ma&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2020,&quot;venue&quot;:&quot;arXiv preprint&quot;,&quot;venue_tags&quot;:[&quot;arxiv&quot;],&quot;type&quot;:[&quot;preprint&quot;],&quot;tags&quot;:[&quot;Sequence analysis&quot;,&quot;Visual analytics&quot;],&quot;highlight&quot;:false,&quot;arxiv&quot;:2006.12718,&quot;image&quot;:&quot;/assets/paper_teaser/2020_ice.png&quot;,&quot;slug&quot;:&quot;2020_ice&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2019_pqe.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2019_pqe&quot;,&quot;path&quot;:&quot;_publications/2019_pqe.html&quot;,&quot;url&quot;:&quot;/publications/2019_pqe&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;],&quot;title&quot;:&quot;A Survey on User Interface Design in Augmented Reality for Real-Time Tasks&quot;,&quot;description&quot;:&quot;The presentation slides of Linping's Ph.D. Qualification Exam.&quot;,&quot;tags&quot;:[&quot;Augmented Reality&quot;,&quot;User Interface Design&quot;],&quot;type&quot;:[&quot;Others&quot;],&quot;year&quot;:2021,&quot;image&quot;:&quot;/assets/paper_teaser/2021_pqe.png&quot;,&quot;slides&quot;:&quot;/assets/slide/2021_pqe_slide.pdf&quot;,&quot;highlight&quot;:false,&quot;slug&quot;:&quot;2019_pqe&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2019_speechlens&quot;,&quot;content&quot;:&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Linping Yuan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019_speechlens\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019_speechlens\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2019_speechlens.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2019_speechlens.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Linping Yuan\&quot;},\&quot;description\&quot;:\&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\&quot;,\&quot;headline\&quot;:\&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2019_speechlens.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019_speechlens\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2019_speechlens.html&quot;,&quot;url&quot;:&quot;/publications/2019_speechlens&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Yuanzhe Chen&quot;,&quot;Siwei Fu&quot;,&quot;Aoyu Wu&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2019,&quot;venue&quot;:&quot;IEEE International Conference on Big Data and Smart Computing&quot;,&quot;venue_location&quot;:&quot;Kyoto, Japan&quot;,&quot;venue_tags&quot;:[&quot;BigComp&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Multimodal analysis&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/BIGCOMP.2019.8679261&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2019_speechlens.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2019_speechlens.pdf&quot;,&quot;demo&quot;:&quot;https://www.youtube.com/watch?v=dtv03qEVFDM&amp;t=1s&quot;,&quot;slug&quot;:&quot;2019_speechlens&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2020_ice&quot;,&quot;content&quot;:&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Siwei Fu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020_ice\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020_ice\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2020_ice.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2020_ice.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Siwei Fu\&quot;},\&quot;description\&quot;:\&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.\&quot;,\&quot;headline\&quot;:\&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2020_ice.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020_ice\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;url&quot;:&quot;/publications/2020_ice&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations&quot;,&quot;authors&quot;:[&quot;Siwei Fu&quot;,&quot;Jian Zhao&quot;,&quot;Linping Yuan&quot;,&quot;Zhicheng Liu&quot;,&quot;Kwan-Liu Ma&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2020,&quot;venue&quot;:&quot;arXiv preprint&quot;,&quot;venue_tags&quot;:[&quot;arxiv&quot;],&quot;type&quot;:[&quot;preprint&quot;],&quot;tags&quot;:[&quot;Sequence analysis&quot;,&quot;Visual analytics&quot;],&quot;highlight&quot;:false,&quot;arxiv&quot;:2006.12718,&quot;image&quot;:&quot;/assets/paper_teaser/2020_ice.png&quot;,&quot;slug&quot;:&quot;2020_ice&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;content&quot;:&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;Deep Colormap Extraction from Visualizations | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Deep Colormap Extraction from Visualizations\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Linping Yuan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Deep Colormap Extraction from Visualizations\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Linping Yuan\&quot;},\&quot;description\&quot;:\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot;,\&quot;headline\&quot;:\&quot;Deep Colormap Extraction from Visualizations\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;url&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;Deep Colormap Extraction from Visualizations&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Wei Zeng&quot;,&quot;Siwei Fu&quot;,&quot;Zhiliang Zeng&quot;,&quot;Haotian Li&quot;,&quot;Chi-Wing Fu&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2021,&quot;venue&quot;:&quot;IEEE Transactions on Visualization and Computer Graphics&quot;,&quot;venue_tags&quot;:[&quot;TVCG&quot;],&quot;type&quot;:[&quot;Journal&quot;],&quot;tags&quot;:[&quot;Color&quot;,&quot;Visualization&quot;,&quot;AI for X&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/TVCG.2021.3070876&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2021_deep_colormap.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2021_deep_colormap.pdf&quot;,&quot;appendix&quot;:&quot;/assets/appendix/2021_deep_colormap_appendix.pdf&quot;,&quot;code&quot;:&quot;https://github.com/yuanlinping/deep_colormap_extraction&quot;,&quot;arxiv&quot;:2103.00741,&quot;slug&quot;:&quot;2021_deep_colormap&quot;,&quot;ext&quot;:&quot;.html&quot;}">
  <div class="thumbnail">
    <img src="/assets/paper_teaser/2021_deep_colormap.png">
  </div>
  <div class="publication-content">
    <h3 id="/publications/2021_deep_colormap">Deep Colormap Extraction from Visualizations</h3>
    <div class="authors">
      <a href="https://yuanlinping.github.io/"><strong>Linping Yuan</strong></a>, 
      <a href="https://zeng-wei.com/">Wei Zeng</a>, 
      <a href="https://fusiwei339.bitbucket.io/">Siwei Fu</a>, 
      Zhiliang Zeng, 
      <a href="https://haotian-li.com/">Haotian Li</a>, 
      <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>, 
      <a href="http://www.huamin.org/">Huamin Qu</a>
      
    </div>
    
      <div class="venue">
        
        IEEE Transactions on Visualization and Computer Graphics (2021)
        
      </div>
    

    

    

    
      <div class="extra-links">
      
        <a target="_blank" href="https://doi.org/10.1109/TVCG.2021.3070876">
            <i class="fas fa-book" aria-hidden="true"></i> DOI
        </a>
      
      
        <a href="/assets/paper/2021_deep_colormap.pdf">
          <i class="far fa-file-pdf" aria-hidden="true"></i> PDF
        </a>
      
      
      <a href="/assets/appendix/2021_deep_colormap_appendix.pdf">
        <i class="far fa-file-pdf" aria-hidden="true"></i> Appendix
      </a>
      
      
      
      <a href="https://github.com/yuanlinping/deep_colormap_extraction">
        <i class="fab fa-github" aria-hidden="true"></i> Code
      </a>
    
      
      
      
      
      
      <a href="https://arxiv.org/abs/2103.00741">
        <i class="fas fa-archive" aria-hidden="true"></i> arXiv
      </a>
      
      
      
      </div>
    
  </div>
</div>

    
  
    
  
    
  
    
      

<div class="publication" data-pub="{&quot;relative_path&quot;:&quot;_publications/2019_speechlens.html&quot;,&quot;excerpt&quot;:&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;excerpt&quot;:&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;excerpt&quot;:&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2021_infocolorizer.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2021_infocolorizer&quot;,&quot;path&quot;:&quot;_publications/2021_infocolorizer.html&quot;,&quot;url&quot;:&quot;/publications/2021_infocolorizer&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;InfoColorizer: Interactive Recommendation of Color Palettes for Infographics&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Ziqi Zhou&quot;,&quot;Jian Zhao&quot;,&quot;Yiqiu Guo&quot;,&quot;Fan Du&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2021,&quot;venue&quot;:&quot;IEEE Transactions on Visualization and Computer Graphics&quot;,&quot;venue_tags&quot;:[&quot;TVCG&quot;],&quot;type&quot;:[&quot;Journal&quot;],&quot;tags&quot;:[&quot;AI for X&quot;,&quot;Graphic design&quot;,&quot;Color&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/TVCG.2021.3085327&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2021_infocolorizer.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2021_infocolorizer.pdf&quot;,&quot;appendix&quot;:&quot;/assets/appendix/2021_infocolorizer_appendix.pdf&quot;,&quot;code&quot;:&quot;https://github.com/yuanlinping/InfoColorizer&quot;,&quot;demo&quot;:&quot;https://www.youtube.com/watch?v=FZvLt0AAIAI&quot;,&quot;arxiv&quot;:2102.02041,&quot;slides&quot;:&quot;/assets/slide/2021_infocolorizer_slide.pdf&quot;,&quot;talk&quot;:&quot;https://www.youtube.com/watch?v=z23Zq5kZruE&quot;,&quot;slug&quot;:&quot;2021_infocolorizer&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2020_ice&quot;,&quot;path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;url&quot;:&quot;/publications/2020_ice&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations&quot;,&quot;authors&quot;:[&quot;Siwei Fu&quot;,&quot;Jian Zhao&quot;,&quot;Linping Yuan&quot;,&quot;Zhicheng Liu&quot;,&quot;Kwan-Liu Ma&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2020,&quot;venue&quot;:&quot;arXiv preprint&quot;,&quot;venue_tags&quot;:[&quot;arxiv&quot;],&quot;type&quot;:[&quot;preprint&quot;],&quot;tags&quot;:[&quot;Sequence analysis&quot;,&quot;Visual analytics&quot;],&quot;highlight&quot;:false,&quot;arxiv&quot;:2006.12718,&quot;image&quot;:&quot;/assets/paper_teaser/2020_ice.png&quot;,&quot;slug&quot;:&quot;2020_ice&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;content&quot;:&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;Deep Colormap Extraction from Visualizations | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Deep Colormap Extraction from Visualizations\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Linping Yuan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Deep Colormap Extraction from Visualizations\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Linping Yuan\&quot;},\&quot;description\&quot;:\&quot;This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\&quot;,\&quot;headline\&quot;:\&quot;Deep Colormap Extraction from Visualizations\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2021_deep_colormap.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2021_deep_colormap\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  This work presents a new approach based on deep learning to automatically extract colormaps from visualizations. After summarizing colors in an input visualization image as a Lab color histogram, we pass the histogram to a pre-trained deep neural network, which learns to predict the colormap that produces the visualization. To train the network, we create a new dataset of ∼64K visualizations that cover various data distributions, chart types, and colormaps. The network adopts an atrous spatial pyramid pooling module to capture color features at multiple scales in the input color histograms. We then classify the predicted colormap as discrete or continuous, and refine the predicted colormap based on its color histogram.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2021_deep_colormap.html&quot;,&quot;url&quot;:&quot;/publications/2021_deep_colormap&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;Deep Colormap Extraction from Visualizations&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Wei Zeng&quot;,&quot;Siwei Fu&quot;,&quot;Zhiliang Zeng&quot;,&quot;Haotian Li&quot;,&quot;Chi-Wing Fu&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2021,&quot;venue&quot;:&quot;IEEE Transactions on Visualization and Computer Graphics&quot;,&quot;venue_tags&quot;:[&quot;TVCG&quot;],&quot;type&quot;:[&quot;Journal&quot;],&quot;tags&quot;:[&quot;Color&quot;,&quot;Visualization&quot;,&quot;AI for X&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/TVCG.2021.3070876&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2021_deep_colormap.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2021_deep_colormap.pdf&quot;,&quot;appendix&quot;:&quot;/assets/appendix/2021_deep_colormap_appendix.pdf&quot;,&quot;code&quot;:&quot;https://github.com/yuanlinping/deep_colormap_extraction&quot;,&quot;arxiv&quot;:2103.00741,&quot;slug&quot;:&quot;2021_deep_colormap&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2019_speechlens.html&quot;,&quot;excerpt&quot;:&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2020_ice&quot;,&quot;path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;url&quot;:&quot;/publications/2020_ice&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations&quot;,&quot;authors&quot;:[&quot;Siwei Fu&quot;,&quot;Jian Zhao&quot;,&quot;Linping Yuan&quot;,&quot;Zhicheng Liu&quot;,&quot;Kwan-Liu Ma&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2020,&quot;venue&quot;:&quot;arXiv preprint&quot;,&quot;venue_tags&quot;:[&quot;arxiv&quot;],&quot;type&quot;:[&quot;preprint&quot;],&quot;tags&quot;:[&quot;Sequence analysis&quot;,&quot;Visual analytics&quot;],&quot;highlight&quot;:false,&quot;arxiv&quot;:2006.12718,&quot;image&quot;:&quot;/assets/paper_teaser/2020_ice.png&quot;,&quot;slug&quot;:&quot;2020_ice&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2019_pqe.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2019_pqe&quot;,&quot;path&quot;:&quot;_publications/2019_pqe.html&quot;,&quot;url&quot;:&quot;/publications/2019_pqe&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;],&quot;title&quot;:&quot;A Survey on User Interface Design in Augmented Reality for Real-Time Tasks&quot;,&quot;description&quot;:&quot;The presentation slides of Linping's Ph.D. Qualification Exam.&quot;,&quot;tags&quot;:[&quot;Augmented Reality&quot;,&quot;User Interface Design&quot;],&quot;type&quot;:[&quot;Others&quot;],&quot;year&quot;:2021,&quot;image&quot;:&quot;/assets/paper_teaser/2021_pqe.png&quot;,&quot;slides&quot;:&quot;/assets/slide/2021_pqe_slide.pdf&quot;,&quot;highlight&quot;:false,&quot;slug&quot;:&quot;2019_pqe&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2019_speechlens&quot;,&quot;content&quot;:&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Linping Yuan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019_speechlens\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019_speechlens\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2019_speechlens.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2019_speechlens.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Linping Yuan\&quot;},\&quot;description\&quot;:\&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\&quot;,\&quot;headline\&quot;:\&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2019_speechlens.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019_speechlens\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2019_speechlens.html&quot;,&quot;url&quot;:&quot;/publications/2019_speechlens&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Yuanzhe Chen&quot;,&quot;Siwei Fu&quot;,&quot;Aoyu Wu&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2019,&quot;venue&quot;:&quot;IEEE International Conference on Big Data and Smart Computing&quot;,&quot;venue_location&quot;:&quot;Kyoto, Japan&quot;,&quot;venue_tags&quot;:[&quot;BigComp&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Multimodal analysis&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/BIGCOMP.2019.8679261&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2019_speechlens.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2019_speechlens.pdf&quot;,&quot;demo&quot;:&quot;https://www.youtube.com/watch?v=dtv03qEVFDM&amp;t=1s&quot;,&quot;slug&quot;:&quot;2019_speechlens&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2020_ice&quot;,&quot;content&quot;:&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Siwei Fu\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020_ice\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020_ice\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2020_ice.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2020_ice.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Siwei Fu\&quot;},\&quot;description\&quot;:\&quot;Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.\&quot;,\&quot;headline\&quot;:\&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2020_ice.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020_ice\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Comparative analysis of event sequence data is essential in many application domains, such as website design and medical care. However, analysts often face two challenges: they may not always know which sets of event sequences in the data are useful to compare, and the comparison needs to be achieved at different granularity, due to the volume and complexity of the data. This paper presents, ICE, an interactive visualization that allows analysts to explore an event sequence dataset, and identify promising sets of event sequences to compare at both the pattern and sequence levels. More specifically, ICE incorporates a multi-level matrix-based visualization for browsing the entire dataset based on the prefixes and suffixes of sequences. To support comparison at multiple levels, ICE employs the unit visualization technique, and we further explore the design space of unit visualizations for event sequence comparison tasks. Finally, we demonstrate the effectiveness of ICE with three real-world datasets from different domains.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;url&quot;:&quot;/publications/2020_ice&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations&quot;,&quot;authors&quot;:[&quot;Siwei Fu&quot;,&quot;Jian Zhao&quot;,&quot;Linping Yuan&quot;,&quot;Zhicheng Liu&quot;,&quot;Kwan-Liu Ma&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2020,&quot;venue&quot;:&quot;arXiv preprint&quot;,&quot;venue_tags&quot;:[&quot;arxiv&quot;],&quot;type&quot;:[&quot;preprint&quot;],&quot;tags&quot;:[&quot;Sequence analysis&quot;,&quot;Visual analytics&quot;],&quot;highlight&quot;:false,&quot;arxiv&quot;:2006.12718,&quot;image&quot;:&quot;/assets/paper_teaser/2020_ice.png&quot;,&quot;slug&quot;:&quot;2020_ice&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2019_pqe.html&quot;,&quot;excerpt&quot;:&quot;Augmented Reality (AR)  is a human-computer interaction technology towards natural interaction and invisible computer interface. By superimposing digital objects in the physical environments, AR can enhance people's perception of the dynamic real world by presenting information that is hard for human senses to perceive and give in-situ action guidance. Thus, various AR applications are designed to assist users in completing urgent tasks in complex circumstances. The user interface is one of the key components of these applications, while its design has had few breakthroughs in the past two decades. These applications have more and more end users nowadays, so it is time to consider their user interface design to provide a better user experience. However, existing works lack an overview of current design practices in existing AR applications, such as the user needs and design considerations. To fill this gap,  we first identify four types of user needs and classify existing applications based on these needs. Then we conduct an in-depth analysis of the user interface design, identify six common design dimensions, and summarize current practices. We conclude with a discussion about limitations and future directions for improving user interface design for AR applications that facilitate real-time tasks.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2019_speechlens.html&quot;,&quot;excerpt&quot;:&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2020_ice&quot;,&quot;path&quot;:&quot;_publications/2020_ice.html&quot;,&quot;url&quot;:&quot;/publications/2020_ice&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix and Unit Visualizations&quot;,&quot;authors&quot;:[&quot;Siwei Fu&quot;,&quot;Jian Zhao&quot;,&quot;Linping Yuan&quot;,&quot;Zhicheng Liu&quot;,&quot;Kwan-Liu Ma&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2020,&quot;venue&quot;:&quot;arXiv preprint&quot;,&quot;venue_tags&quot;:[&quot;arxiv&quot;],&quot;type&quot;:[&quot;preprint&quot;],&quot;tags&quot;:[&quot;Sequence analysis&quot;,&quot;Visual analytics&quot;],&quot;highlight&quot;:false,&quot;arxiv&quot;:2006.12718,&quot;image&quot;:&quot;/assets/paper_teaser/2020_ice.png&quot;,&quot;slug&quot;:&quot;2020_ice&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2019_pqe.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2019_pqe&quot;,&quot;path&quot;:&quot;_publications/2019_pqe.html&quot;,&quot;url&quot;:&quot;/publications/2019_pqe&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;],&quot;title&quot;:&quot;A Survey on User Interface Design in Augmented Reality for Real-Time Tasks&quot;,&quot;description&quot;:&quot;The presentation slides of Linping's Ph.D. Qualification Exam.&quot;,&quot;tags&quot;:[&quot;Augmented Reality&quot;,&quot;User Interface Design&quot;],&quot;type&quot;:[&quot;Others&quot;],&quot;year&quot;:2021,&quot;image&quot;:&quot;/assets/paper_teaser/2021_pqe.png&quot;,&quot;slides&quot;:&quot;/assets/slide/2021_pqe_slide.pdf&quot;,&quot;highlight&quot;:false,&quot;slug&quot;:&quot;2019_pqe&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2019_speechlens&quot;,&quot;content&quot;:&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Linping Yuan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019_speechlens\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019_speechlens\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2019_speechlens.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2019_speechlens.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Linping Yuan\&quot;},\&quot;description\&quot;:\&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\&quot;,\&quot;headline\&quot;:\&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2019_speechlens.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019_speechlens\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2019_speechlens.html&quot;,&quot;url&quot;:&quot;/publications/2019_speechlens&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Yuanzhe Chen&quot;,&quot;Siwei Fu&quot;,&quot;Aoyu Wu&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2019,&quot;venue&quot;:&quot;IEEE International Conference on Big Data and Smart Computing&quot;,&quot;venue_location&quot;:&quot;Kyoto, Japan&quot;,&quot;venue_tags&quot;:[&quot;BigComp&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Multimodal analysis&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/BIGCOMP.2019.8679261&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2019_speechlens.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2019_speechlens.pdf&quot;,&quot;demo&quot;:&quot;https://www.youtube.com/watch?v=dtv03qEVFDM&amp;t=1s&quot;,&quot;slug&quot;:&quot;2019_speechlens&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:{&quot;relative_path&quot;:&quot;_publications/2018_stagemap.html&quot;,&quot;excerpt&quot;:&quot;Temporal event sequences are becoming increasingly important in many application domains such as website click streams, user interaction logs, electronic health records and car service records. A real-world dataset with a large number of event sequences of varying lengths is complex and difficult to analyze. To support visual exploration of the data, it is desirable yet challenging to provide a concise and meaningful overview of sequences. In this paper, we focus on the stage, that is, a frequently occurring subsequence in the dataset. We introduce StageMap, a novel visualization technique to summarize event sequence data into a set of stage progression patterns. The resulting overview is more concise compared with event-level summarization and supports level-of-detail exploration. We further present a visual analytics system with four linked views, which are overview, tree view, stage view and sequences view. We also present case studies and discuss advantages and limitations of applying StageMap to real-world scenarios.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;next&quot;:{&quot;relative_path&quot;:&quot;_publications/2019_pqe.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;id&quot;:&quot;/publications/2019_pqe&quot;,&quot;path&quot;:&quot;_publications/2019_pqe.html&quot;,&quot;url&quot;:&quot;/publications/2019_pqe&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;],&quot;title&quot;:&quot;A Survey on User Interface Design in Augmented Reality for Real-Time Tasks&quot;,&quot;description&quot;:&quot;The presentation slides of Linping's Ph.D. Qualification Exam.&quot;,&quot;tags&quot;:[&quot;Augmented Reality&quot;,&quot;User Interface Design&quot;],&quot;type&quot;:[&quot;Others&quot;],&quot;year&quot;:2021,&quot;image&quot;:&quot;/assets/paper_teaser/2021_pqe.png&quot;,&quot;slides&quot;:&quot;/assets/slide/2021_pqe_slide.pdf&quot;,&quot;highlight&quot;:false,&quot;slug&quot;:&quot;2019_pqe&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;previous&quot;:null,&quot;id&quot;:&quot;/publications/2018_stagemap&quot;,&quot;content&quot;:&quot;Temporal event sequences are becoming increasingly important in many application domains such as website click streams, user interaction logs, electronic health records and car service records. A real-world dataset with a large number of event sequences of varying lengths is complex and difficult to analyze. To support visual exploration of the data, it is desirable yet challenging to provide a concise and meaningful overview of sequences. In this paper, we focus on the stage, that is, a frequently occurring subsequence in the dataset. We introduce StageMap, a novel visualization technique to summarize event sequence data into a set of stage progression patterns. The resulting overview is more concise compared with event-level summarization and supports level-of-detail exploration. We further present a visual analytics system with four linked views, which are overview, tree view, stage view and sequences view. We also present case studies and discuss advantages and limitations of applying StageMap to real-world scenarios.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;StageMap: Extracting and Summarizing Progression Stages in Event Sequences | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;StageMap: Extracting and Summarizing Progression Stages in Event Sequences\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Yuanzhe Chen\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Temporal event sequences are becoming increasingly important in many application domains such as website click streams, user interaction logs, electronic health records and car service records. A real-world dataset with a large number of event sequences of varying lengths is complex and difficult to analyze. To support visual exploration of the data, it is desirable yet challenging to provide a concise and meaningful overview of sequences. In this paper, we focus on the stage, that is, a frequently occurring subsequence in the dataset. We introduce StageMap, a novel visualization technique to summarize event sequence data into a set of stage progression patterns. The resulting overview is more concise compared with event-level summarization and supports level-of-detail exploration. We further present a visual analytics system with four linked views, which are overview, tree view, stage view and sequences view. We also present case studies and discuss advantages and limitations of applying StageMap to real-world scenarios.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Temporal event sequences are becoming increasingly important in many application domains such as website click streams, user interaction logs, electronic health records and car service records. A real-world dataset with a large number of event sequences of varying lengths is complex and difficult to analyze. To support visual exploration of the data, it is desirable yet challenging to provide a concise and meaningful overview of sequences. In this paper, we focus on the stage, that is, a frequently occurring subsequence in the dataset. We introduce StageMap, a novel visualization technique to summarize event sequence data into a set of stage progression patterns. The resulting overview is more concise compared with event-level summarization and supports level-of-detail exploration. We further present a visual analytics system with four linked views, which are overview, tree view, stage view and sequences view. We also present case studies and discuss advantages and limitations of applying StageMap to real-world scenarios.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018_stagemap\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018_stagemap\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2018_stagemap.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2018_stagemap.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;StageMap: Extracting and Summarizing Progression Stages in Event Sequences\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Yuanzhe Chen\&quot;},\&quot;description\&quot;:\&quot;Temporal event sequences are becoming increasingly important in many application domains such as website click streams, user interaction logs, electronic health records and car service records. A real-world dataset with a large number of event sequences of varying lengths is complex and difficult to analyze. To support visual exploration of the data, it is desirable yet challenging to provide a concise and meaningful overview of sequences. In this paper, we focus on the stage, that is, a frequently occurring subsequence in the dataset. We introduce StageMap, a novel visualization technique to summarize event sequence data into a set of stage progression patterns. The resulting overview is more concise compared with event-level summarization and supports level-of-detail exploration. We further present a visual analytics system with four linked views, which are overview, tree view, stage view and sequences view. We also present case studies and discuss advantages and limitations of applying StageMap to real-world scenarios.\&quot;,\&quot;headline\&quot;:\&quot;StageMap: Extracting and Summarizing Progression Stages in Event Sequences\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2018_stagemap.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018_stagemap\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Temporal event sequences are becoming increasingly important in many application domains such as website click streams, user interaction logs, electronic health records and car service records. A real-world dataset with a large number of event sequences of varying lengths is complex and difficult to analyze. To support visual exploration of the data, it is desirable yet challenging to provide a concise and meaningful overview of sequences. In this paper, we focus on the stage, that is, a frequently occurring subsequence in the dataset. We introduce StageMap, a novel visualization technique to summarize event sequence data into a set of stage progression patterns. The resulting overview is more concise compared with event-level summarization and supports level-of-detail exploration. We further present a visual analytics system with four linked views, which are overview, tree view, stage view and sequences view. We also present case studies and discuss advantages and limitations of applying StageMap to real-world scenarios.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2018_stagemap.html&quot;,&quot;url&quot;:&quot;/publications/2018_stagemap&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;StageMap: Extracting and Summarizing Progression Stages in Event Sequences&quot;,&quot;authors&quot;:[&quot;Yuanzhe Chen&quot;,&quot;Abishek Puri&quot;,&quot;Linping Yuan&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2018,&quot;venue&quot;:&quot;IEEE International Conference on Big Data&quot;,&quot;venue_location&quot;:&quot;Seattle, WA, USA&quot;,&quot;venue_tags&quot;:[&quot;Big Data&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Sequence analysis&quot;,&quot;Visual analytics&quot;],&quot;highlight&quot;:false,&quot;doi&quot;:&quot;10.1109/BigData.2018.8622571&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2018_stagemap.png&quot;,&quot;slug&quot;:&quot;2018_stagemap&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2019_pqe&quot;,&quot;content&quot;:&quot;Augmented Reality (AR)  is a human-computer interaction technology towards natural interaction and invisible computer interface. By superimposing digital objects in the physical environments, AR can enhance people's perception of the dynamic real world by presenting information that is hard for human senses to perceive and give in-situ action guidance. Thus, various AR applications are designed to assist users in completing urgent tasks in complex circumstances. The user interface is one of the key components of these applications, while its design has had few breakthroughs in the past two decades. These applications have more and more end users nowadays, so it is time to consider their user interface design to provide a better user experience. However, existing works lack an overview of current design practices in existing AR applications, such as the user needs and design considerations. To fill this gap,  we first identify four types of user needs and classify existing applications based on these needs. Then we conduct an in-depth analysis of the user interface design, identify six common design dimensions, and summarize current practices. We conclude with a discussion about limitations and future directions for improving user interface design for AR applications that facilitate real-time tasks.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;A Survey on User Interface Design in Augmented Reality for Real-Time Tasks | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A Survey on User Interface Design in Augmented Reality for Real-Time Tasks\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Linping Yuan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;The presentation slides of Linping’s Ph.D. Qualification Exam.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;The presentation slides of Linping’s Ph.D. Qualification Exam.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019_pqe\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019_pqe\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_pqe.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2021_pqe.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A Survey on User Interface Design in Augmented Reality for Real-Time Tasks\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Linping Yuan\&quot;},\&quot;description\&quot;:\&quot;The presentation slides of Linping’s Ph.D. Qualification Exam.\&quot;,\&quot;headline\&quot;:\&quot;A Survey on User Interface Design in Augmented Reality for Real-Time Tasks\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2021_pqe.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019_pqe\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Augmented Reality (AR)  is a human-computer interaction technology towards natural interaction and invisible computer interface. By superimposing digital objects in the physical environments, AR can enhance people's perception of the dynamic real world by presenting information that is hard for human senses to perceive and give in-situ action guidance. Thus, various AR applications are designed to assist users in completing urgent tasks in complex circumstances. The user interface is one of the key components of these applications, while its design has had few breakthroughs in the past two decades. These applications have more and more end users nowadays, so it is time to consider their user interface design to provide a better user experience. However, existing works lack an overview of current design practices in existing AR applications, such as the user needs and design considerations. To fill this gap,  we first identify four types of user needs and classify existing applications based on these needs. Then we conduct an in-depth analysis of the user interface design, identify six common design dimensions, and summarize current practices. We conclude with a discussion about limitations and future directions for improving user interface design for AR applications that facilitate real-time tasks.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2019_pqe.html&quot;,&quot;url&quot;:&quot;/publications/2019_pqe&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;],&quot;title&quot;:&quot;A Survey on User Interface Design in Augmented Reality for Real-Time Tasks&quot;,&quot;description&quot;:&quot;The presentation slides of Linping's Ph.D. Qualification Exam.&quot;,&quot;tags&quot;:[&quot;Augmented Reality&quot;,&quot;User Interface Design&quot;],&quot;type&quot;:[&quot;Others&quot;],&quot;year&quot;:2021,&quot;image&quot;:&quot;/assets/paper_teaser/2021_pqe.png&quot;,&quot;slides&quot;:&quot;/assets/slide/2021_pqe_slide.pdf&quot;,&quot;highlight&quot;:false,&quot;slug&quot;:&quot;2019_pqe&quot;,&quot;ext&quot;:&quot;.html&quot;},&quot;id&quot;:&quot;/publications/2019_speechlens&quot;,&quot;content&quot;:&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.&quot;,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en-US\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot;&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot;&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot;&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot;&gt;\n  &lt;meta name=\&quot;keywords\&quot; content=\&quot;袁林萍, 林萍, Yuan Linping, Linping Yuan, Linping, yuan linping, linping yuan, linping, YUAN Linping, Linping YUAN, yuanlinping\&quot;&gt;\n  &lt;meta name=\&quot;description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;title&gt;Linping YUAN&lt;/title&gt;\n  &lt;meta property=\&quot;og:title\&quot; content=\&quot;Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:site_name\&quot; content=\&quot;http://localhost:4000 Linping YUAN\&quot;&gt;\n  &lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot;&gt;\n  &lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000\&quot;&gt;\n  &lt;meta property=\&quot;og:image\&quot; content=\&quot;\&quot;&gt;\n  &lt;meta property=\&quot;og:description\&quot; content=\&quot;Personal website of Linping Yuan (袁林萍), a Ph.D. candidate in Hong Kong University of Science and Technology.\&quot;&gt;\n  &lt;meta name=\&quot;author\&quot; content=\&quot;Linping YUAN (袁林萍)\&quot;&gt;\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot;/&gt;\n  &lt;link rel=\&quot;icon\&quot; type=\&quot;image/png\&quot; href=\&quot;/favicon.png\&quot; sizes=\&quot;250x250\&quot; /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.8.0 --&gt;\n&lt;title&gt;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features | Linping YUAN&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.2\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Linping Yuan\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019_speechlens\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019_speechlens\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;Linping YUAN\&quot; /&gt;\n&lt;meta property=\&quot;og:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2019_speechlens.png\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;website\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary_large_image\&quot; /&gt;\n&lt;meta property=\&quot;twitter:image\&quot; content=\&quot;http://localhost:4000/assets/paper_teaser/2019_speechlens.png\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;@context\&quot;:\&quot;https://schema.org\&quot;,\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Linping Yuan\&quot;},\&quot;description\&quot;:\&quot;Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\&quot;,\&quot;headline\&quot;:\&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features\&quot;,\&quot;image\&quot;:\&quot;http://localhost:4000/assets/paper_teaser/2019_speechlens.png\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019_speechlens\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n\n  &lt;link rel=\&quot;alternate\&quot; type=\&quot;application/rss+xml\&quot; title=\&quot;Linping YUAN\&quot; href=\&quot;http://localhost:4000/feed.xml\&quot;&gt;\n\n  &lt;link href=\&quot;https://use.fontawesome.com/releases/v5.15.3/css/all.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot;&gt;\n&lt;/head&gt;\n  \n  &lt;body&gt;\n    \n&lt;header class=\&quot;page-header\&quot;&gt;\n  &lt;nav class=\&quot;container\&quot;&gt;\n    &lt;a class=\&quot;site-title\&quot; href=\&quot;/\&quot;&gt;Linping YUAN&lt;/a&gt;\n\n    &lt;a href=\&quot;/publications/\&quot; class=\&quot;active\&quot;&gt;Publications&lt;/a&gt;\n    &lt;a href=\&quot;/projects/\&quot; &gt;Projects&lt;/a&gt;\n    &lt;a href=\&quot;/cv/\&quot; &gt;CV&lt;/a&gt;\n\n    &lt;span class=\&quot;external\&quot;&gt;\n      &lt;a href=\&quot;https://yuanlinping.github.io/blog\&quot;&gt; Blog&lt;/a&gt;\n    &lt;/span&gt;\n  &lt;/nav&gt;\n&lt;/header&gt;\n\n\n    &lt;div class=\&quot;page-content\&quot;&gt;\n      &lt;section class=\&quot;container \&quot;&gt;\n  Public speaking is an effective way to move, persuade, and inspire. While many guidelines have been presented to teach public speaking skills, they are often based on anecdotal evidence and not customizable. Exploring highquality speeches such as TED Talks could provide insights to eliminate limitations in existing guidelines. This study aims to explore and identify narration strategies by conducting visual analysis into the textural and acoustic information in public speeches. We present SpeechLens, an interactive visual analytics system to explore large-scale speech dataset with multiple level of details. SpeechLens features a novel focus+context design to enable intuitive and smooth analysis. Case studies indicate the effectiveness and usefulness of our approach.\n&lt;/section&gt;\n\n    &lt;/div&gt;\n\n    &lt;footer&gt;\n  &lt;div class=\&quot;container\&quot;&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Greetings from Hong Kong. &lt;br/&gt;All Rights Reserved.&lt;br/&gt;\n      &lt;abbr title=\&quot;Last build on 2022-09-29\&quot;&gt;September 2022&lt;/abbr&gt;\n    &lt;/div&gt;\n    &lt;div class=\&quot;footer-col site-desc\&quot;&gt;I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.\n&lt;/div&gt;\n    &lt;div class=\&quot;footer-col\&quot;&gt;\n      Template from &lt;br /&gt;&lt;a target=\&quot;_blank\&quot; href=\&quot;https://github.com/domoritz/domoritz.github.io\&quot;&gt;@domoritz&lt;/a&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/footer&gt;\n    &lt;script&gt;\n  function trim(str) {\n    return str.replace(/^\\s+|\\s+$/g, '');\n  }\n  var headers = document.querySelectorAll(\&quot;h2, h3, h4, h5, h6\&quot;);\n  for (var i=0; i&lt;headers.length; i++) {\n    var h = headers[i];\n    var name = h.getAttribute(\&quot;id\&quot;);\n    var title = h.innerHTML;\n    h.innerHTML = '&lt;a href=\&quot;#' + name + '\&quot; class=\&quot;anchor\&quot;&gt;&lt;i class=\&quot;fas fa-hashtag\&quot;&gt;&lt;/i&gt;&lt;/a&gt;' + trim(title);\n  }\n&lt;/script&gt;\n\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;path&quot;:&quot;_publications/2019_speechlens.html&quot;,&quot;url&quot;:&quot;/publications/2019_speechlens&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;title&quot;:&quot;SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features&quot;,&quot;authors&quot;:[&quot;Linping Yuan&quot;,&quot;Yuanzhe Chen&quot;,&quot;Siwei Fu&quot;,&quot;Aoyu Wu&quot;,&quot;Huamin Qu&quot;],&quot;year&quot;:2019,&quot;venue&quot;:&quot;IEEE International Conference on Big Data and Smart Computing&quot;,&quot;venue_location&quot;:&quot;Kyoto, Japan&quot;,&quot;venue_tags&quot;:[&quot;BigComp&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Multimodal analysis&quot;],&quot;highlight&quot;:true,&quot;doi&quot;:&quot;10.1109/BIGCOMP.2019.8679261&quot;,&quot;image&quot;:&quot;/assets/paper_teaser/2019_speechlens.png&quot;,&quot;pdf&quot;:&quot;/assets/paper/2019_speechlens.pdf&quot;,&quot;demo&quot;:&quot;https://www.youtube.com/watch?v=dtv03qEVFDM&amp;t=1s&quot;,&quot;slug&quot;:&quot;2019_speechlens&quot;,&quot;ext&quot;:&quot;.html&quot;}">
  <div class="thumbnail">
    <img src="/assets/paper_teaser/2019_speechlens.png">
  </div>
  <div class="publication-content">
    <h3 id="/publications/2019_speechlens">SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features</h3>
    <div class="authors">
      <a href="https://yuanlinping.github.io/"><strong>Linping Yuan</strong></a>, 
      Yuanzhe Chen, 
      <a href="https://fusiwei339.bitbucket.io/">Siwei Fu</a>, 
      <a href="https://awuac.student.ust.hk/">Aoyu Wu</a>, 
      <a href="http://www.huamin.org/">Huamin Qu</a>
      
    </div>
    
      <div class="venue">
        
          IEEE International Conference on Big Data and Smart Computing in Kyoto, Japan 2019
        
      </div>
    

    

    

    
      <div class="extra-links">
      
        <a target="_blank" href="https://doi.org/10.1109/BIGCOMP.2019.8679261">
            <i class="fas fa-book" aria-hidden="true"></i> DOI
        </a>
      
      
        <a href="/assets/paper/2019_speechlens.pdf">
          <i class="far fa-file-pdf" aria-hidden="true"></i> PDF
        </a>
      
      
      
      
      
      
        <a href="https://www.youtube.com/watch?v=dtv03qEVFDM&amp;t=1s">
          <i class="fas fa-play" aria-hidden="true"></i> Demo
        </a>
      
      
      
      
      
      
      </div>
    
  </div>
</div>

    
  
    
  
</div>

<!-- style 2: simple; don't delete-->
<!-- <div class="featured-publications">
  
  
    
      <div class="publication pubs">
        
        <a href="https://doi.org/10.1109/TVCG.2021.3085327" target="_blank"><span class="pub-title">InfoColorizer: Interactive Recommendation of Color Palettes for Infographics</span>.</a>
        
        <div class="authors">
          
            <a href="https://yuanlinping.github.io/" target="_blank"><span class="author_me">Linping Yuan</span></a>
            , 
          
            Ziqi Zhou
            , 
          
            <a href="http://www.jeffjianzhao.com/" target="_blank">Jian Zhao</a>
            , 
          
            Yiqiu Guo
            , 
          
            <a href="http://frankdu.org/" target="_blank">Fan Du</a>
            , 
          
            <a href="http://www.huamin.org/" target="_blank">Huamin Qu</a>
            
          .
          <br/><i>IEEE Transactions on Visualization and Computer Graphics</i>, 2021.
          
          
        </div>
      </div>
    
  
    
      <div class="publication pubs">
        
        <a href="https://doi.org/10.1109/TVCG.2021.3070876" target="_blank"><span class="pub-title">Deep Colormap Extraction from Visualizations</span>.</a>
        
        <div class="authors">
          
            <a href="https://yuanlinping.github.io/" target="_blank"><span class="author_me">Linping Yuan</span></a>
            , 
          
            <a href="https://zeng-wei.com/" target="_blank">Wei Zeng</a>
            , 
          
            <a href="https://fusiwei339.bitbucket.io/" target="_blank">Siwei Fu</a>
            , 
          
            Zhiliang Zeng
            , 
          
            <a href="https://haotian-li.com/" target="_blank">Haotian Li</a>
            , 
          
            <a href="https://www.cse.cuhk.edu.hk/~cwfu/" target="_blank">Chi-Wing Fu</a>
            , 
          
            <a href="http://www.huamin.org/" target="_blank">Huamin Qu</a>
            
          .
          <br/><i>IEEE Transactions on Visualization and Computer Graphics</i>, 2021.
          
          
        </div>
      </div>
    
  
    
  
    
  
    
      <div class="publication pubs">
        
        <a href="https://doi.org/10.1109/BIGCOMP.2019.8679261" target="_blank"><span class="pub-title">SpeechLens: A Visual Analytics Approach for Exploring Speech Strategies with Textural and Acoustic Features</span>.</a>
        
        <div class="authors">
          
            <a href="https://yuanlinping.github.io/" target="_blank"><span class="author_me">Linping Yuan</span></a>
            , 
          
            Yuanzhe Chen
            , 
          
            <a href="https://fusiwei339.bitbucket.io/" target="_blank">Siwei Fu</a>
            , 
          
            <a href="https://awuac.student.ust.hk/" target="_blank">Aoyu Wu</a>
            , 
          
            <a href="http://www.huamin.org/" target="_blank">Huamin Qu</a>
            
          .
          <br/><i>IEEE International Conference on Big Data and Smart Computing</i>, 2019.
          
          
        </div>
      </div>
    
  
    
  
</div> -->

<p><a href="/publications/" class="button">
  <i class="fas fa-chevron-circle-right"></i>
  Show All Publications
</a></p>

</section>

    </div>

    <footer>
  <div class="container">
    <div class="footer-col">
      Greetings from Hong Kong. <br>All Rights Reserved.<br>
      <abbr title="Last build on 2022-09-29">September 2022</abbr>
    </div>
    <div class="footer-col site-desc">I am a Ph.D. candidate at the Hong Kong University of Science and Technology (HKUST). I work on augmented reality, virtual reality, human-computer interaction, and data visualization.
</div>
    <div class="footer-col">
      Template from <br><a target="_blank" href="https://github.com/domoritz/domoritz.github.io">@domoritz</a>
    </div>
  </div>
</footer>
    <script>
  function trim(str) {
    return str.replace(/^\s+|\s+$/g, '');
  }
  var headers = document.querySelectorAll("h2, h3, h4, h5, h6");
  for (var i=0; i<headers.length; i++) {
    var h = headers[i];
    var name = h.getAttribute("id");
    var title = h.innerHTML;
    h.innerHTML = '<a href="#' + name + '" class="anchor"><i class="fas fa-hashtag"></i></a>' + trim(title);
  }
</script>

  </body>
</html>
